{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68ef2cb",
   "metadata": {},
   "source": [
    "# Extract Customers Data to Bronze Layer\n",
    "\n",
    "This notebook extracts customer data from the WorldWideImporters SQL Server database and loads it into the Unity Catalog bronze layer.\n",
    "\n",
    "## Parameters\n",
    "- `catalog_name`: Unity Catalog name\n",
    "- `schema_name`: Schema name (bronze)\n",
    "- `sql_server_host`: SQL Server hostname\n",
    "- `sql_database_name`: SQL Database name\n",
    "- `sql_username`: SQL Server username\n",
    "- `sql_password`: SQL Server password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Extract Customers Data to Bronze Layer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Parameters\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters from widget or job parameters\n",
    "dbutils.widgets.text(\"catalog_name\", \"don_datalab_catalog\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"bronze\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"sql_server_host\", \"\", \"SQL Server Host\")\n",
    "dbutils.widgets.text(\"sql_database_name\", \"WorldWideImporters\", \"SQL Database Name\")\n",
    "dbutils.widgets.text(\"sql_username\", \"\", \"SQL Username\")\n",
    "dbutils.widgets.text(\"sql_password\", \"\", \"SQL Password\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "sql_server_host = dbutils.widgets.get(\"sql_server_host\")\n",
    "sql_database_name = dbutils.widgets.get(\"sql_database_name\")\n",
    "sql_username = dbutils.widgets.get(\"sql_username\")\n",
    "sql_password = dbutils.widgets.get(\"sql_password\")\n",
    "\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "print(f\"Schema: {schema_name}\")\n",
    "print(f\"SQL Server: {sql_server_host}\")\n",
    "print(f\"Database: {sql_database_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Setup Unity Catalog\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create catalog if not exists\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "\n",
    "# Create schema if not exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "# Use the catalog and schema\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"Using catalog: {catalog_name}, schema: {schema_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Extract Customer Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configure SQL Server connection\n",
    "jdbc_url = f\"jdbc:sqlserver://{sql_server_host}:1433;database={sql_database_name};encrypt=true;trustServerCertificate=true\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": sql_username,\n",
    "    \"password\": sql_password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# SQL query to extract customer data\n",
    "customer_query = \"\"\"\n",
    "SELECT \n",
    "    CustomerID,\n",
    "    CustomerName,\n",
    "    BillToCustomerID,\n",
    "    CustomerCategoryID,\n",
    "    PrimaryContactPersonID,\n",
    "    DeliveryMethodID,\n",
    "    DeliveryCityID,\n",
    "    PostalCityID,\n",
    "    AccountOpenedDate,\n",
    "    StandardDiscountPercentage,\n",
    "    IsStatementSent,\n",
    "    IsOnCreditHold,\n",
    "    PaymentDays,\n",
    "    PhoneNumber,\n",
    "    FaxNumber,\n",
    "    WebsiteURL,\n",
    "    DeliveryAddressLine1,\n",
    "    DeliveryPostalCode,\n",
    "    PostalAddressLine1,\n",
    "    PostalPostalCode,\n",
    "    LastEditedBy,\n",
    "    ValidFrom,\n",
    "    ValidTo\n",
    "FROM Sales.Customers\n",
    "\"\"\"\n",
    "\n",
    "# Read data from SQL Server\n",
    "try:\n",
    "    customers_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=f\"({customer_query}) as customers\",\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully extracted {customers_df.count()} customer records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting customer data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Add Metadata and Load to Bronze\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Add metadata columns\n",
    "customers_bronze = customers_df \\\n",
    "    .withColumn(\"extraction_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", lit(\"worldwideimporters_sql\")) \\\n",
    "    .withColumn(\"source_table\", lit(\"Sales.Customers\")) \\\n",
    "    .withColumn(\"bronze_layer_version\", lit(\"1.0\"))\n",
    "\n",
    "# Show sample data\n",
    "print(\"Sample customer data:\")\n",
    "customers_bronze.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "customers_bronze.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Write to Unity Catalog bronze layer\n",
    "table_name = f\"{catalog_name}.{schema_name}.customers\"\n",
    "\n",
    "try:\n",
    "    customers_bronze.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    print(f\"Successfully loaded customer data to {table_name}\")\n",
    "    \n",
    "    # Verify the load\n",
    "    record_count = spark.table(table_name).count()\n",
    "    print(f\"Verified: {record_count} records in {table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer data to Unity Catalog: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Basic data quality checks\n",
    "total_records = spark.table(table_name).count()\n",
    "null_customer_names = spark.table(table_name).filter(col(\"CustomerName\").isNull()).count()\n",
    "duplicate_customer_ids = spark.table(table_name).groupBy(\"CustomerID\").count().filter(col(\"count\") > 1).count()\n",
    "\n",
    "print(f\"Data Quality Report for {table_name}:\")\n",
    "print(f\"- Total records: {total_records}\")\n",
    "print(f\"- Records with null CustomerName: {null_customer_names}\")\n",
    "print(f\"- Duplicate CustomerIDs: {duplicate_customer_ids}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"‚úÖ Customer data extraction completed successfully!\")\n",
    "print(f\"üìä Loaded {total_records} customer records to {table_name}\")\n",
    "print(f\"‚è∞ Extraction completed at: {datetime.datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
